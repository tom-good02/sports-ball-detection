{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics yt-dlp opencv-python pillow matplotlib pandas numpy requests"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YOX84EuwvOO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import cv2\n",
        "import json\n",
        "import yt_dlp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "from scipy.interpolate import splprep, splev\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_project_structure():\n",
        "  base_dir = '/content/sports_ball_detection'\n",
        "  directories = [\n",
        "    'data/raw_videos',\n",
        "    'data/frames',\n",
        "    'data/frames_augmented',\n",
        "    'data/annotations',\n",
        "    'data/yolo_dataset/images/train',\n",
        "    'data/yolo_dataset/images/val',\n",
        "    'data/yolo_dataset/labels/train',\n",
        "    'data/yolo_dataset/labels/val',\n",
        "    'results/visualisations',\n",
        "    'results/metrics'\n",
        "  ]\n",
        "\n",
        "  for dir_path in directories:\n",
        "    full_path = os.path.join(base_dir, dir_path)\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "  return base_dir\n",
        "\n",
        "def get_dataset_metadata(num):\n",
        "  # Create a temporary directory for the data\n",
        "  os.makedirs('/content/temp', exist_ok=True)\n",
        "\n",
        "  # Clone the repository to get the metadata\n",
        "  !git clone https://github.com/gtoderici/sports-1m-dataset.git /content/temp/sports-1m-dataset\n",
        "\n",
        "  # Read the labels file\n",
        "  with open('/content/temp/sports-1m-dataset/labels.txt', 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Read the training partition file\n",
        "  with open('/content/temp/sports-1m-dataset/original/train_partition.txt', 'r') as f:\n",
        "      train_videos = [line.strip().split(' ') for line in f.readlines()]\n",
        "\n",
        "  # Convert to a more manageable format\n",
        "  video_data = []\n",
        "  for video in train_videos[:num]:  # Only process first N for speed\n",
        "    if len(video) >= 2:\n",
        "      url, labels_str = video\n",
        "      video_id = url.split('=')[-1]\n",
        "      labels_list = [int(l) for l in labels_str.split(',')]\n",
        "      video_data.append({\n",
        "        'id': video_id,\n",
        "        'label487': labels_list,\n",
        "        'stitle': f\"Video {video_id}\",\n",
        "        'labels_text': [labels[l] for l in labels_list]\n",
        "      })\n",
        "\n",
        "  return video_data\n",
        "\n",
        "def download_video(video_id, output_path, max_retries=3):\n",
        "  \"\"\"\n",
        "  Download video from youtube using yt-dlp\n",
        "  \"\"\"\n",
        "  url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "  output_template = os.path.join(output_path, f\"{video_id}.mp4\")\n",
        "\n",
        "  # First verify if video exists\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "      print(f\"Video {video_id} is not available (Status code: {response.status_code})\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "    print(f\"Error checking video availability for {video_id}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "  # Configure yt-dlp options\n",
        "  ydl_opts = {\n",
        "    'format': 'worst[ext=mp4]',  # Get lowest quality to save time\n",
        "    'outtmpl': output_template,\n",
        "    'quiet': True,\n",
        "    'no_warnings': True,\n",
        "    'ignoreerrors': True,\n",
        "    'nocheckcertificate': True,\n",
        "    'extract_flat': False,\n",
        "    'socket_timeout': 30,\n",
        "  }\n",
        "\n",
        "  for attempt in range(max_retries):\n",
        "    try:\n",
        "      with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "\n",
        "      if os.path.exists(output_template):\n",
        "        return output_template\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Attempt {attempt + 1}/{max_retries} failed for video {video_id}: {str(e)}\")\n",
        "      time.sleep(2 * (attempt + 1))  # Exponential backoff\n",
        "\n",
        "      if attempt == max_retries - 1:\n",
        "        print(f\"Failed to download video {video_id} after {max_retries} attempts\")\n",
        "        return None\n",
        "\n",
        "  return None\n",
        "\n",
        "def extract_frames(video_path, output_dir, sample_rate=1):\n",
        "  \"\"\"\n",
        "  Extract frames from video at given sample rate (1 frame every N seconds)\n",
        "  \"\"\"\n",
        "  if not os.path.exists(video_path):\n",
        "    return []\n",
        "\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frame_interval = int(fps * sample_rate)\n",
        "  frame_paths = []\n",
        "\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  frame_count = 0\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    if frame_count % frame_interval == 0:\n",
        "      frame_path = os.path.join(\n",
        "        output_dir,\n",
        "        f\"{os.path.basename(video_path)}_{frame_count}.jpg\"\n",
        "      )\n",
        "      cv2.imwrite(frame_path, frame)\n",
        "      frame_paths.append(frame_path)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "  cap.release()\n",
        "  return frame_paths\n",
        "\n",
        "def create_initial_dataset(video_data, base_dir, num_videos=5, max_attempts=10):\n",
        "  \"\"\"\n",
        "  Create a initial dataset\n",
        "  \"\"\"\n",
        "  filtered_videos = []\n",
        "  for video in video_data:\n",
        "    if any('ball' in label.lower() for label in video['labels_text']): # select videos with 'ball' in label\n",
        "      filtered_videos.append(video)\n",
        "\n",
        "  if not filtered_videos:\n",
        "    raise ValueError(\"No videos found matching the sports criteria\")\n",
        "\n",
        "  dataset_info = []\n",
        "  attempts = 0\n",
        "  processed_video_ids = set()\n",
        "\n",
        "  # Keep trying until we get the desired number of videos or reach max attempts\n",
        "  while len(dataset_info) < num_videos and attempts < max_attempts:\n",
        "    # Get random videos we haven't tried yet\n",
        "    remaining_videos = [v for v in filtered_videos if v['id'] not in processed_video_ids]\n",
        "    if not remaining_videos:\n",
        "      print(\"No more videos available to try\")\n",
        "      break\n",
        "\n",
        "    selected_video = random.choice(remaining_videos)\n",
        "    video_id = selected_video['id']\n",
        "    processed_video_ids.add(video_id)\n",
        "\n",
        "    print(f\"\\nAttempting to process video {video_id} ({len(dataset_info) + 1}/{num_videos})\")\n",
        "\n",
        "    try:\n",
        "      video_path = download_video(video_id, os.path.join(base_dir, 'data/raw_videos'))\n",
        "      if video_path:\n",
        "        frames_dir = os.path.join(base_dir, 'data/frames', video_id)\n",
        "        print(frames_dir)\n",
        "        frame_paths = extract_frames(video_path, frames_dir)\n",
        "\n",
        "        if frame_paths:\n",
        "          dataset_info.append({\n",
        "            'video_id': video_id,\n",
        "            'frame_paths': frame_paths,\n",
        "            'labels': selected_video['labels_text'],\n",
        "            'label_ids': selected_video['label487']\n",
        "          })\n",
        "          print(f\"Successfully processed video {video_id}\")\n",
        "        else:\n",
        "          print(f\"No frames could be extracted from video {video_id}\")\n",
        "      else:\n",
        "        print(f\"Failed to download video {video_id}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing video {video_id}: {str(e)}\")\n",
        "    finally:\n",
        "      # Clean up video file\n",
        "      try:\n",
        "          os.remove(video_path)\n",
        "          print(f\"Cleaned up video file: {video_path}\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error cleaning up video file {video_path}: {str(e)}\")\n",
        "\n",
        "    attempts += 1\n",
        "\n",
        "  print(f\"\\nFinished processing. Successfully downloaded {len(dataset_info)} videos after {attempts} attempts\")\n",
        "  return dataset_info\n",
        "\n",
        "class BallDetectionMetrics:\n",
        "    def __init__(self, base_dir):\n",
        "      self.metrics_history = []\n",
        "      self.base_dir = base_dir\n",
        "\n",
        "    def calculate_metrics(self, model):\n",
        "      \"\"\"Calculate precision, recall, and mAP for the model\"\"\"\n",
        "      dataset_yaml = os.path.join(self.base_dir, 'data', 'yolo_dataset', 'dataset.yaml')\n",
        "\n",
        "      if not os.path.exists(dataset_yaml):\n",
        "        raise ValueError(\"No dataset.yaml found. Please ensure you have valid data before training.\")\n",
        "\n",
        "      results = model.val(data=dataset_yaml)\n",
        "      metrics = {\n",
        "        'mAP50-95': results.box.map,\n",
        "        'mAP50': results.box.map50,\n",
        "        'mAP75': results.box.map75\n",
        "      }\n",
        "      self.metrics_history.append(metrics)\n",
        "      return metrics\n",
        "\n",
        "    def plot_metrics_history(self, save_path):\n",
        "        \"\"\"Plot metrics history over iterations\"\"\"\n",
        "        metrics = pd.DataFrame(self.metrics_history)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        for column in metrics.columns:\n",
        "          plt.plot(metrics[column], label=column)\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Model Performance Over Iterations')\n",
        "        plt.legend()\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "class BallDetectionPipeline:\n",
        "  def __init__(self, base_dir):\n",
        "    self.base_dir = base_dir\n",
        "    self.model = YOLO('yolov8x.pt', verbose=False)\n",
        "    self.ball_classes = [32]  # 'sports ball' class\n",
        "    self.metrics = BallDetectionMetrics(base_dir=base_dir)\n",
        "\n",
        "  def generate_pseudo_annotations(self, dataset_info):\n",
        "    \"\"\"Generate annotations using YOLOv8 pretrained model\"\"\"\n",
        "    annotations = []\n",
        "\n",
        "    for video in dataset_info:\n",
        "      print(f\"Processing video {video['video_id']}\")\n",
        "      video_annotations = []\n",
        "      for frame_path in video['frame_paths']:\n",
        "        results = self.model(frame_path, verbose=False)\n",
        "\n",
        "        for r in results:\n",
        "          boxes = r.boxes\n",
        "          for box in boxes:\n",
        "            if int(box.cls) in self.ball_classes:\n",
        "              x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "              img = Image.open(frame_path)\n",
        "              w, h = img.size\n",
        "              x_center = (x1 + x2) / (2 * w)\n",
        "              y_center = (y1 + y2) / (2 * h)\n",
        "              width = (x2 - x1) / w\n",
        "              height = (y2 - y1) / h\n",
        "\n",
        "              video_annotations.append({\n",
        "                'frame_path': frame_path,\n",
        "                'bbox': [x_center, y_center, width, height],\n",
        "                'confidence': float(box.conf),\n",
        "                'class': int(box.cls)\n",
        "              })\n",
        "\n",
        "      if video_annotations:\n",
        "        annotations.append({\n",
        "            'video_id': video['video_id'],\n",
        "            'annotations': video_annotations\n",
        "        })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "  def save_yolo_annotations(self, annotations, confidence_threshold=0.5, is_validation=False):\n",
        "    \"\"\"Save annotations in YOLO format with train/val split\"\"\"\n",
        "    if not annotations:\n",
        "      print(\"No annotations to save. Skipping dataset creation.\")\n",
        "      return False\n",
        "\n",
        "    dataset_dir = os.path.join(self.base_dir, 'data', 'yolo_dataset')\n",
        "    split = 'val' if is_validation else 'train'\n",
        "\n",
        "    yaml_path = os.path.join(dataset_dir, 'dataset.yaml')\n",
        "\n",
        "    saved_any_images = False\n",
        "\n",
        "    for video_data in annotations:\n",
        "      video_id = video_data['video_id']\n",
        "\n",
        "      video_img_dir = os.path.join(dataset_dir, 'images', split, str(video_id))\n",
        "      video_label_dir = os.path.join(dataset_dir, 'labels', split, str(video_id))\n",
        "      os.makedirs(video_img_dir, exist_ok=True)\n",
        "      os.makedirs(video_label_dir, exist_ok=True)\n",
        "\n",
        "      high_conf_annotations = [ann for ann in video_data['annotations']\n",
        "                            if ann['confidence'] >= confidence_threshold]\n",
        "\n",
        "      if high_conf_annotations:\n",
        "        saved_any_images = True\n",
        "        for ann in high_conf_annotations:\n",
        "          img_path = Path(ann['frame_path'])\n",
        "          new_img_path = os.path.join(video_img_dir, img_path.name)\n",
        "          os.system(f'cp {img_path} {new_img_path}')\n",
        "\n",
        "          label_path = os.path.join(video_label_dir, img_path.stem + '.txt')\n",
        "          with open(label_path, 'w') as f:\n",
        "            f.write(f\"0 {' '.join(map(str, ann['bbox']))}\\n\")\n",
        "\n",
        "    if saved_any_images:\n",
        "      if not os.path.exists(yaml_path):\n",
        "        yaml_content = {\n",
        "          'path': dataset_dir,\n",
        "          'train': 'images/train',\n",
        "          'val': 'images/val',\n",
        "          'names': {0: 'sports_ball'}\n",
        "        }\n",
        "        with open(yaml_path, 'w') as f:\n",
        "          json.dump(yaml_content, f, indent=2)\n",
        "    return saved_any_images\n",
        "\n",
        "  def train_model(self, epochs=100):\n",
        "    \"\"\"Train YOLOv8 model on our dataset\"\"\"\n",
        "    dataset_yaml = os.path.join(self.base_dir, 'data', 'yolo_dataset', 'dataset.yaml')\n",
        "\n",
        "    if not os.path.exists(dataset_yaml):\n",
        "      raise ValueError(\"No dataset.yaml found. Please ensure you have valid data before training.\")\n",
        "\n",
        "    images_dir = os.path.join(self.base_dir, 'data', 'yolo_dataset', 'images')\n",
        "    if not os.path.exists(images_dir) or not any(os.scandir(images_dir)):\n",
        "      raise ValueError(\"No training data found. Please ensure you have valid images before training.\")\n",
        "\n",
        "    self.model.train(\n",
        "      data=dataset_yaml,\n",
        "      epochs=epochs,\n",
        "      imgsz=640,\n",
        "      batch=16,\n",
        "      name='ball_detection',\n",
        "      verbose=False\n",
        "    )\n",
        "\n",
        "  def visualise_detections(self, annotations, num_frames=5, save_dir=None):\n",
        "    \"\"\"\n",
        "    Visualise ball detections on frames with bounding boxes, organised by video ID\n",
        "    \"\"\"\n",
        "    if save_dir is None:\n",
        "      save_dir = os.path.join(self.base_dir, 'results', 'visualisations')\n",
        "\n",
        "    # Process each video separately\n",
        "    saved_paths = []\n",
        "\n",
        "    for video_data in annotations:\n",
        "      video_id = video_data['video_id']\n",
        "      video_save_dir = os.path.join(save_dir, str(video_id))\n",
        "      os.makedirs(video_save_dir, exist_ok=True)\n",
        "\n",
        "      # Sort frames by confidence for this video\n",
        "      frames_with_balls = sorted(\n",
        "        video_data['annotations'],\n",
        "        key=lambda x: x['confidence'],\n",
        "        reverse=True\n",
        "      )[:num_frames]\n",
        "\n",
        "      for i, detection in enumerate(frames_with_balls):\n",
        "        # Read the image\n",
        "        frame_path = detection['frame_path']\n",
        "        image = cv2.imread(frame_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get image dimensions\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Convert YOLO format to pixel coordinates\n",
        "        x_center, y_center, box_width, box_height = detection['bbox']\n",
        "        x_center *= width\n",
        "        y_center *= height\n",
        "        box_width *= width\n",
        "        box_height *= height\n",
        "\n",
        "        # Calculate box corners\n",
        "        x1 = int(x_center - box_width/2)\n",
        "        y1 = int(y_center - box_height/2)\n",
        "        x2 = int(x_center + box_width/2)\n",
        "        y2 = int(y_center + box_height/2)\n",
        "\n",
        "        # Draw the bounding box\n",
        "        color = (255, 0, 0)  # Red\n",
        "        thickness = 2\n",
        "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "        # Add confidence score and video ID\n",
        "        conf_text = f\"Conf:{detection['confidence']:.2f}\"\n",
        "        cv2.putText(image, conf_text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, thickness)\n",
        "\n",
        "        # Save the image\n",
        "        save_path = os.path.join(video_save_dir, f'detection_{i+1}.jpg')\n",
        "        image_pil = Image.fromarray(image)\n",
        "        image_pil.save(save_path)\n",
        "        saved_paths.append(save_path)\n",
        "\n",
        "        print(f\"Saved visualisation for video {video_id}: {i+1}/{len(frames_with_balls)} to {save_path}\")\n",
        "\n",
        "    return saved_paths\n",
        "\n",
        "  def generate_detection_video(self, video_path, output_path, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Generate a video with ball detection boxes overlaid on original footage\n",
        "    \"\"\"\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Setup video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Tracking detections across frames\n",
        "    frame_count = 0\n",
        "    total_detections = []\n",
        "\n",
        "    # Progress tracking\n",
        "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
        "    print(f\"Total frames: {total_frames}\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Detect balls in the current frame\n",
        "        results = self.model(frame, verbose=False)\n",
        "\n",
        "        # Store detections for this frame\n",
        "        frame_detections = []\n",
        "\n",
        "        for r in results:\n",
        "            boxes = r.boxes\n",
        "            for box in boxes:\n",
        "                # Check if detection is a ball and meets confidence threshold\n",
        "                if (int(box.cls) in self.ball_classes and\n",
        "                    float(box.conf) >= confidence_threshold):\n",
        "\n",
        "                    # Convert YOLO format to pixel coordinates\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "                    confidence = float(box.conf)\n",
        "\n",
        "                    # Draw bounding box\n",
        "                    color = (0, 255, 0)  # Green color\n",
        "                    thickness = 2\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "                    # Add confidence text\n",
        "                    text = f'Ball: {confidence:.2f}'\n",
        "                    cv2.putText(frame, text, (x1, y1-10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "                    # Store detection details\n",
        "                    frame_detections.append({\n",
        "                        'frame': frame_count,\n",
        "                        'bbox': [x1, y1, x2, y2],\n",
        "                        'confidence': confidence\n",
        "                    })\n",
        "\n",
        "        # Write annotated frame to output video\n",
        "        out.write(frame)\n",
        "\n",
        "        # Store detections\n",
        "        if frame_detections:\n",
        "            total_detections.append({\n",
        "                'frame': frame_count,\n",
        "                'detections': frame_detections\n",
        "            })\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress update\n",
        "        if frame_count % 100 == 0:\n",
        "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
        "\n",
        "    # Release video objects\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Generate detection summary\n",
        "    print(\"\\nDetection Summary:\")\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Frames with ball detections: {len(total_detections)}\")\n",
        "\n",
        "    # Optional: Save detection details to a JSON for further analysis\n",
        "    detection_summary_path = output_path.replace('.mp4', '_detections.json')\n",
        "    with open(detection_summary_path, 'w') as f:\n",
        "        json.dump(total_detections, f, indent=2)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "class DatasetImprovement:\n",
        "    def __init__(self, pipeline, base_dir):\n",
        "      self.pipeline = pipeline\n",
        "      self.base_dir = base_dir\n",
        "\n",
        "    def cleanup_iteration_data(self):\n",
        "      \"\"\"Clean up the YOLO dataset directory between iterations\"\"\"\n",
        "      dataset_dir = os.path.join(self.base_dir, 'data', 'yolo_dataset')\n",
        "      paths_to_clean = [\n",
        "        os.path.join(dataset_dir, 'images', 'train'),\n",
        "        os.path.join(dataset_dir, 'images', 'val'),\n",
        "        os.path.join(dataset_dir, 'labels', 'train'),\n",
        "        os.path.join(dataset_dir, 'labels', 'val')\n",
        "      ]\n",
        "\n",
        "      for path in paths_to_clean:\n",
        "        if os.path.exists(path):\n",
        "          shutil.rmtree(path)\n",
        "          os.makedirs(path)  # Recreate empty directory\n",
        "\n",
        "      # Also clean up augmented frames\n",
        "      aug_dir = os.path.join(self.base_dir, 'data', 'frames_augmented')\n",
        "      if os.path.exists(aug_dir):\n",
        "        shutil.rmtree(aug_dir)\n",
        "        os.makedirs(aug_dir)\n",
        "\n",
        "    def split_dataset(self, dataset_info, val_split=0.2):\n",
        "      \"\"\"Split dataset into training and validation sets\"\"\"\n",
        "      random.shuffle(dataset_info)\n",
        "      split_idx = int(len(dataset_info) * (1 - val_split))\n",
        "      return dataset_info[:split_idx], dataset_info[split_idx:]\n",
        "\n",
        "    def identify_easy_cases(self, annotations, confidence_threshold=0.8):\n",
        "      \"\"\"Identify frames where model is most confident\"\"\"\n",
        "      easy_cases = []\n",
        "      for video in annotations:\n",
        "        video_easy_cases = [\n",
        "          ann for ann in video['annotations']\n",
        "          if confidence_threshold <= ann['confidence'] <= 1.0\n",
        "        ]\n",
        "        if video_easy_cases:\n",
        "          easy_cases.append({\n",
        "            'video_id': video['video_id'],\n",
        "            'annotations': video_easy_cases\n",
        "          })\n",
        "      return easy_cases\n",
        "\n",
        "    def augment_annotations(self, annotations, confidence_threshold=0.5):\n",
        "      \"\"\"\n",
        "      Augment high-confidence detections with their corresponding annotations\n",
        "      Returns new annotations in YOLO format\n",
        "      \"\"\"\n",
        "      augmented_annotations = []\n",
        "\n",
        "      for video_data in annotations:\n",
        "        video_id = video_data['video_id']\n",
        "        high_conf_detections = [\n",
        "          ann for ann in video_data['annotations']\n",
        "          if ann['confidence'] >= confidence_threshold\n",
        "        ]\n",
        "\n",
        "        if not high_conf_detections:\n",
        "          continue\n",
        "\n",
        "        aug_annotations = []\n",
        "\n",
        "        for detection in high_conf_detections:\n",
        "          # Read original image\n",
        "          img = cv2.imread(detection['frame_path'])\n",
        "          if img is None:\n",
        "            continue\n",
        "\n",
        "          original_h, original_w = img.shape[:2]\n",
        "          bbox = detection['bbox']  # [x_center, y_center, width, height] in normalised coordinates\n",
        "\n",
        "          # Create augmentations\n",
        "          augmentations = [\n",
        "            ('bright', self._brightness_augmentation(img)),\n",
        "            ('flip', self._horizontal_flip_augmentation(img))\n",
        "          ]\n",
        "\n",
        "          for aug_name, aug_img in augmentations:\n",
        "            # Create augmented image filename and path\n",
        "            base_name = os.path.basename(detection['frame_path'])\n",
        "            aug_img_name = f\"{os.path.splitext(base_name)[0]}_{aug_name}.jpg\"\n",
        "\n",
        "            # Save augmented image to YOLO dataset directory\n",
        "            aug_img_path = os.path.join(\n",
        "              self.base_dir, 'data', 'yolo_dataset',\n",
        "              'images', 'train', video_id,\n",
        "              aug_img_name\n",
        "            )\n",
        "            os.makedirs(os.path.dirname(aug_img_path), exist_ok=True)\n",
        "            cv2.imwrite(aug_img_path, aug_img)\n",
        "\n",
        "            # Calculate augmented bbox\n",
        "            if aug_name == 'flip':\n",
        "              # For horizontal flip, adjust x coordinate\n",
        "              aug_bbox = [\n",
        "                  1.0 - bbox[0],  # flip x_center\n",
        "                  bbox[1],\n",
        "                  bbox[2],\n",
        "                  bbox[3]\n",
        "              ]\n",
        "            else:\n",
        "              # For brightness, keep same bbox\n",
        "              aug_bbox = bbox.copy()\n",
        "\n",
        "            # Add augmented annotation\n",
        "            aug_annotations.append({\n",
        "              'frame_path': aug_img_path,\n",
        "              'bbox': aug_bbox,\n",
        "              'confidence': detection['confidence'],\n",
        "              'class': detection['class']\n",
        "            })\n",
        "\n",
        "        if aug_annotations:\n",
        "          augmented_annotations.append({\n",
        "            'video_id': f\"{video_id}_aug\",\n",
        "            'annotations': aug_annotations\n",
        "          })\n",
        "\n",
        "      return augmented_annotations\n",
        "\n",
        "    def _brightness_augmentation(self, img):\n",
        "      \"\"\"Apply brightness augmentation\"\"\"\n",
        "      return cv2.convertScaleAbs(img, alpha=1.2, beta=10)\n",
        "\n",
        "    def _horizontal_flip_augmentation(self, img):\n",
        "      \"\"\"Apply horizontal flip augmentation\"\"\"\n",
        "      return cv2.flip(img, 1)\n",
        "\n",
        "    def iterative_improvement(self, initial_dataset, num_iterations=3):\n",
        "      \"\"\"Iteratively improve the dataset and model\"\"\"\n",
        "\n",
        "      for iteration in range(num_iterations):\n",
        "        print(f\"\\nIteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "        # Clean up data from previous iteration\n",
        "        print(\"\\nCleaning up previous iteration data...\")\n",
        "        self.cleanup_iteration_data()\n",
        "        print(\"Cleaned up\")\n",
        "\n",
        "        # Split dataset\n",
        "        print(\"\\nSplitting dataset...\")\n",
        "        train_data, val_data = self.split_dataset(initial_dataset)\n",
        "        print(\"Split\")\n",
        "\n",
        "        print(f\"\\nTraining data size: {len(train_data)}\")\n",
        "        print(f\"Validation data size: {len(val_data)}\")\n",
        "        print(\"\\nTraining data\")\n",
        "        for data in train_data:\n",
        "          print(f\"Video ID: {data['video_id']}, Labels: {data['labels']}\")\n",
        "\n",
        "        print(\"\\nValidation data\")\n",
        "        for data in val_data:\n",
        "          print(f\"Video ID: {data['video_id']}, Labels: {data['labels']}\")\n",
        "\n",
        "        # Generate annotations\n",
        "        print(\"\\nGenerating annotations...\")\n",
        "        train_annotations = self.pipeline.generate_pseudo_annotations(train_data)\n",
        "        val_annotations = self.pipeline.generate_pseudo_annotations(val_data)\n",
        "        print(f\"Generated {len(train_annotations)} training annotations and {len(val_annotations)} validation annotations\")\n",
        "\n",
        "        # Save annotations and train model\n",
        "        print(\"\\nSaving annotations...\")\n",
        "        self.pipeline.save_yolo_annotations(train_annotations)\n",
        "        self.pipeline.save_yolo_annotations(val_annotations, is_validation=True)\n",
        "        print(f\"Saved annotations\")\n",
        "\n",
        "        # Generate and save augmented annotations from high-confidence detections\n",
        "        print(\"\\nGenerating augmented annotations...\")\n",
        "        augmented_annotations = self.augment_annotations(train_annotations)\n",
        "        if augmented_annotations:\n",
        "          print(f\"Generated {len(augmented_annotations)} augmented annotations\")\n",
        "          print(\"Saving augmented annotations...\")\n",
        "          self.pipeline.save_yolo_annotations(augmented_annotations)\n",
        "        else:\n",
        "          print(\"No high-confidence detections found for augmentation\")\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nTraining model...\")\n",
        "        self.pipeline.train_model(epochs=2)\n",
        "        print(f\"Model trained for iteration {iteration + 1}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(\"\\nCalculating metrics\")\n",
        "        metrics = self.pipeline.metrics.calculate_metrics(self.pipeline.model)\n",
        "        print(f\"Metrics for iteration {iteration + 1}:\", metrics)\n",
        "\n",
        "      # Plot final metrics\n",
        "      self.pipeline.metrics.plot_metrics_history(\n",
        "        os.path.join(self.base_dir, 'results', 'metrics', 'metrics_history.png')\n",
        "      )\n",
        "\n",
        "def complete_cleanup(base_dir):\n",
        "  \"\"\"Clean up all temporary and iteration-specific data\"\"\"\n",
        "  # Clean up YOLO dataset directories\n",
        "  dataset_dir = os.path.join(base_dir, 'data', 'yolo_dataset')\n",
        "  paths_to_clean = [\n",
        "    os.path.join(dataset_dir, 'images', 'train'),\n",
        "    os.path.join(dataset_dir, 'images', 'val'),\n",
        "    os.path.join(dataset_dir, 'labels', 'train'),\n",
        "    os.path.join(dataset_dir, 'labels', 'val')\n",
        "  ]\n",
        "\n",
        "  for path in paths_to_clean:\n",
        "    if os.path.exists(path):\n",
        "      shutil.rmtree(path)\n",
        "      os.makedirs(path)  # Recreate empty directory\n",
        "\n",
        "  # Clean up augmented frames\n",
        "  aug_dir = os.path.join(base_dir, 'data', 'frames_augmented')\n",
        "  if os.path.exists(aug_dir):\n",
        "    shutil.rmtree(aug_dir)\n",
        "    os.makedirs(aug_dir)\n",
        "\n",
        "  # Clean up YOLO training results\n",
        "  runs_dir = os.path.join(base_dir, 'runs')\n",
        "  if os.path.exists(runs_dir):\n",
        "    shutil.rmtree(runs_dir)\n",
        "\n",
        "  # Clean up results directory\n",
        "  results_dirs = [\n",
        "    os.path.join(base_dir, 'results', 'visualisations'),\n",
        "    os.path.join(base_dir, 'results', 'metrics')\n",
        "  ]\n",
        "  for dir_path in results_dirs:\n",
        "    if os.path.exists(dir_path):\n",
        "      shutil.rmtree(dir_path)\n",
        "      os.makedirs(dir_path)\n",
        "\n",
        "  # Remove dataset.yaml if it exists\n",
        "  yaml_path = os.path.join(dataset_dir, 'dataset.yaml')\n",
        "  if os.path.exists(yaml_path):\n",
        "    os.remove(yaml_path)\n",
        "\n",
        "# def run_me_first():\n",
        "  # # Create the project structure\n",
        "  # base_dir = create_project_structure()\n",
        "  # print(f\"Project directory created at: {base_dir}\")\n",
        "\n",
        "  # # Get the dataset metadata\n",
        "  # video_data = get_dataset_metadata(num=1000)\n",
        "  # print(f\"Retrieved metadata for {len(video_data)} videos\")\n",
        "\n",
        "  # # Create initial dataset\n",
        "  # dataset_info = create_initial_dataset(video_data, base_dir, num_videos=20, max_attempts=30)\n",
        "  # print(f\"\\nInitial dataset created with {len(dataset_info)} videos\")\n",
        "\n",
        "  # return base_dir, video_data, dataset_info\n",
        "\n",
        "# def run_me_second(base_dir, video_data, dataset_info):\n",
        "#   complete_cleanup(base_dir)\n",
        "\n",
        "#   # Initialise the ball detection pipeline\n",
        "#   pipeline = BallDetectionPipeline(base_dir)\n",
        "#   print(\"\\nBall detection pipeline initialised\")\n",
        "\n",
        "#   # Initialise the improvement pipeline\n",
        "#   improvement_pipeline = DatasetImprovement(pipeline, base_dir)\n",
        "#   print(\"\\nDataset improvement pipeline initialised\")\n",
        "\n",
        "#   # Run the iterative improvement process\n",
        "#   print(\"\\nStarting iterative improvement process...\")\n",
        "#   improvement_pipeline.iterative_improvement(dataset_info, num_iterations=2)\n",
        "#   print(\"\\nIterative improvement complete!\")\n",
        "\n",
        "#   # Generate final annotations\n",
        "#   print(\"\\nGenerating final annotations...\")\n",
        "#   final_annotations = pipeline.generate_pseudo_annotations(dataset_info)\n",
        "\n",
        "#   # Generate detection visualisations\n",
        "#   print(\"\\nGenerating detection visualisations...\")\n",
        "#   visualisation_paths = pipeline.visualise_detections(final_annotations, num_frames=5)\n",
        "#   print(f\"Created {len(visualisation_paths)} visualisation images\")\n",
        "\n",
        "#   print(\"\\nTraining and evaluation complete!\")\n",
        "#   print(f\"Results saved in: {os.path.join(base_dir, 'results')}\")"
      ],
      "metadata": {
        "id": "LVdm-AAV2M8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base_dir, video_data, dataset_info = run_me_first()\n",
        "\n",
        "\n",
        "# Create the project structure\n",
        "base_dir = create_project_structure()\n",
        "print(f\"Project directory created at: {base_dir}\")\n",
        "\n",
        "# Get the dataset metadata\n",
        "video_data = get_dataset_metadata(num=1000)\n",
        "print(f\"Retrieved metadata for {len(video_data)} videos\")\n",
        "\n",
        "# Create initial dataset\n",
        "dataset_info = create_initial_dataset(video_data, base_dir, num_videos=10, max_attempts=30)\n",
        "print(f\"\\nInitial dataset created with {len(dataset_info)} videos\")"
      ],
      "metadata": {
        "id": "uT1_z9O1mDWx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_me_second(base_dir, video_data, dataset_info)\n",
        "\n",
        "\n",
        "complete_cleanup(base_dir)\n",
        "\n",
        "# Initialise the ball detection pipeline\n",
        "pipeline = BallDetectionPipeline(base_dir)\n",
        "print(\"\\nBall detection pipeline initialised\")\n",
        "\n",
        "# Initialise the improvement pipeline\n",
        "improvement_pipeline = DatasetImprovement(pipeline, base_dir)\n",
        "print(\"\\nDataset improvement pipeline initialised\")\n",
        "\n",
        "# Run the iterative improvement process\n",
        "print(\"\\nStarting iterative improvement process...\")\n",
        "improvement_pipeline.iterative_improvement(dataset_info, num_iterations=2)\n",
        "print(\"\\nIterative improvement complete!\")\n",
        "\n",
        "# Generate final annotations\n",
        "print(\"\\nGenerating final annotations...\")\n",
        "final_annotations = pipeline.generate_pseudo_annotations(dataset_info)\n",
        "\n",
        "print(\"\\nTraining and evaluation complete!\")\n",
        "print(f\"Results saved in: {os.path.join(base_dir, 'results')}\")"
      ],
      "metadata": {
        "id": "psCmPuo1rS68",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate detection visualisations\n",
        "print(\"\\nGenerating detection visualisations...\")\n",
        "visualisation_paths = pipeline.visualise_detections(final_annotations, num_frames=5)\n",
        "print(f\"Created {len(visualisation_paths)} visualisation images\")\n",
        "\n",
        "print(f\"Results saved in: {os.path.join(base_dir, 'results', 'visualisations')}\")"
      ],
      "metadata": {
        "id": "GgsAGTCtgFJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a video to annotate\n",
        "video_id = 'JF2_Evit478'\n",
        "\n",
        "# Download video\n",
        "print(f\"Downloading video: {video_id}\")\n",
        "video_path = download_video(video_id, os.path.join(base_dir, 'data/raw_videos'))\n",
        "print(f\"Downloaded video: {video_path}\")\n",
        "\n",
        "# Create output directory for detection videos\n",
        "print(\"Creating output directory for detection videos...\")\n",
        "detection_video_dir = os.path.join(base_dir, 'results', 'detection_videos')\n",
        "os.makedirs(detection_video_dir, exist_ok=True)\n",
        "print(\"Output directory created\")\n",
        "\n",
        "# Generate output video path\n",
        "print(\"Generating output video path...\")\n",
        "output_video_path = os.path.join(detection_video_dir, f\"{video_id}_ball_detection.mp4\")\n",
        "print(\"Output video path generated\")\n",
        "\n",
        "# Generate video with ball detections\n",
        "print(\"Generating video with ball detections...\")\n",
        "pipeline.generate_detection_video(\n",
        "    video_path=video_path,\n",
        "    output_path=output_video_path,\n",
        "    confidence_threshold=0.5\n",
        ")\n",
        "\n",
        "# Clean raw video\n",
        "print(\"Cleaning raw video...\")\n",
        "os.remove(video_path)\n",
        "print(\"Raw video cleaned\")\n",
        "\n",
        "print(\"Video with ball detections generated\")\n",
        "print(\"Saved video to:\", output_video_path)"
      ],
      "metadata": {
        "id": "m4HvPBz88VCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!zip -r '/content/file.zip' '/content/sports_ball_detection'"
      ],
      "metadata": {
        "id": "6i-LKQFGcFZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "id": "BLqAS2THcG8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}